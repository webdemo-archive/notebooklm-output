1
00:00:00,000 --> 00:00:06,500
さて、今回は2025年の生成AIと人材開発というテーマでして

2
00:00:06,500 --> 00:00:13,000
専門家の分析をまとめた資料を、えっと、リスナーの皆さんと一緒に深掘りしていきたいと思います。

3
00:00:13,000 --> 00:00:14,500
はい。

4
00:00:14,500 --> 00:00:20,000
ただですね、今日注目したいのは、個別の技術がどう進化したかという話ではなくて

5
00:00:20,000 --> 00:00:21,500
ええ。

6
00:00:21,500 --> 00:00:25,000
もっと大きな、何というか地殻変動のような変化なんです。

7
00:00:25,000 --> 00:00:31,000
資料を読み解くと、もう技術が実験段階じゃなくて、社会に実装されるフェーズに入ってきて

8
00:00:31,000 --> 00:00:32,500
そうですね。

9
00:00:32,500 --> 00:00:36,500
その中で、現場では「効率化」とか「教育」、

10
00:00:36,500 --> 00:00:41,000
それから「ルール」に「選択肢」、そして「表現方法」。

11
00:00:41,000 --> 00:00:46,000
この5つの側面で、同時に大きな変化が起きていることが見えてくるんですよね。

12
00:00:46,000 --> 00:00:51,500
ええ、まさにその「地殻変動」という言葉がぴったりだと思いますね。

13
00:00:51,500 --> 00:00:52,500
はい。

14
00:00:52,500 --> 00:00:58,000
ここで面白いのは、資料が示唆しているように、主役はもはや「AI」っていう技術そのものではない、という点なんです。

15
00:00:58,000 --> 00:01:00,500
あ、技術そのものではない？

16
00:01:00,500 --> 00:01:06,000
ええ、むしろこれまで企業が当たり前としてきた、例えば「社員は一斉に集めて教えるものだ」とか、

17
00:01:06,000 --> 00:01:13,000
「教材は専門家が作るものだ」みたいな、そういう常識がですね、根底から覆され始めていると。

18
00:01:13,000 --> 00:01:14,000
なるほど。

19
00:01:14,000 --> 00:01:19,000
その構造変化こそが、今回の議論の、まあ核心になりますね。

20
00:01:19,000 --> 00:01:25,000
これからお話しする5つの大きなニュースは、その地殻変動が引き起こした具体的な現象と言えるでしょう。

21
00:01:25,000 --> 00:01:26,000
なるほど。

22
00:01:26,000 --> 00:01:30,000
ではその最初のプレートの動きから見ていきましょうか。

23
00:01:30,000 --> 00:01:36,000
「全社員リスキリングが人的資本戦略の本丸になった」というニュースです。

24
00:01:36,000 --> 00:01:37,000
はい。

25
00:01:37,000 --> 00:01:42,000
資料にあるNTTデータグループの事例、これはとにかく規模がすごいですよね。

26
00:01:42,000 --> 00:01:49,000
全社員約20万人を対象に生成AIの育成を始めて、目標を大幅に前倒しで達成したと。

27
00:01:49,000 --> 00:01:50,000
ええ。

28
00:01:50,000 --> 00:01:54,000
ソフトバンクも大規模な社内コンテストを続けているそうですし、

29
00:01:54,000 --> 00:02:00,000
これはもう何というか、一部のITに詳しい人だけの話では完全になくなった、ということですよね。

30
00:02:00,000 --> 00:02:01,000
まさしく。

31
00:02:01,000 --> 00:02:08,000
数年前まで「DX人材」っていう、まあある種の専門職のスキルだったものが、

32
00:02:08,000 --> 00:02:12,000
今や全社員が持つべき基礎教養、

33
00:02:12,000 --> 00:02:16,500
いわば「読み書きそろばん」のようなレベルになった。

34
00:02:16,500 --> 00:02:18,500
読み書きそろばん、ですか。

35
00:02:18,500 --> 00:02:23,000
ええ、これが2025年に起きた最大の変化ですね。

36
00:02:23,000 --> 00:02:29,000
で、重要なのは、ただ「研修ビデオを見せて終わり」ではない点なんです。

37
00:02:29,000 --> 00:02:30,000
はい。

38
00:02:30,000 --> 00:02:37,000
資料にもあるように、ホワイトベルトとかイエローベルトといったスキルレベルを明確に定義して、

39
00:02:37,000 --> 00:02:44,000
職種ごとに「あなたはこのレベルを目指してくださいね」と、認定制度まで設計する動きが本格化してるんです。

40
00:02:44,000 --> 00:02:45,500
なるほど。

41
00:02:45,500 --> 00:02:52,000
ただ学びましょうって呼びかけるだけじゃなくて、ちゃんと具体的な目標と物差しを用意したと。

42
00:02:52,000 --> 00:02:58,000
でもこれだけ大規模な投資をするからには、企業側にも相当の狙いがあるはずですよね。

43
00:02:58,000 --> 00:03:03,000
その人的資本経営との結びつきっていうのは、具体的にどういうことなんでしょう？

44
00:03:03,000 --> 00:03:04,000
はい。

45
00:03:04,000 --> 00:03:10,000
単に20万人研修しましたってIR資料に書いても、投資家は「それで？」ってなりませんか。

46
00:03:10,000 --> 00:03:12,000
ああ、いい指摘ですね。

47
00:03:12,000 --> 00:03:18,500
そこがまさに核心でして、投資家が今見ているのは、研修の人数じゃなくて「事業への貢献」なんです。

48
00:03:18,500 --> 00:03:20,000
事業への貢献？

49
00:03:20,000 --> 00:03:24,000
ですから、IRで語られるストーリーはこうなります。

50
00:03:24,000 --> 00:03:30,000
「我が社では営業職のイエローベルト認定者を5000人育成しました。

51
00:03:30,000 --> 00:03:38,500
その結果、彼らがAIを活用して作成した提案書の受注率が平均で15%向上し、これだけの利益増につながりました」と。

52
00:03:38,500 --> 00:03:39,500
なるほど。

53
00:03:39,500 --> 00:03:47,000
ここまで具体的に「人材への投資がどうリターンを生んだか」を説明することが求められる時代になった。

54
00:03:47,000 --> 00:03:54,500
つまり社員教育が、企業の価値を測るための明確なKPIになったわけです。

55
00:03:54,500 --> 00:03:56,000
いや、腑に落ちました。

56
00:03:56,000 --> 00:04:04,000
社員のスキルが、工場の設備とか特許と同じように、企業価値を構成する重要な資産として語られるようになったんですね。

57
00:04:04,000 --> 00:04:05,500
その通りです。

58
00:04:05,500 --> 00:04:10,000
そうなると、20万人もの社員を効率よく、かつ効果的に教育する必要が出てきますよね。

59
00:04:10,000 --> 00:04:13,500
従来の一斉研修では時間もコストも追いつかない。

60
00:04:13,500 --> 00:04:18,500
当然、学び方そのものにイノベーションが求められるわけで、

61
00:04:18,500 --> 00:04:22,000
それが2つ目のニュースにつながってきそうですね。

62
00:04:22,000 --> 00:04:23,500
おっしゃる通りです。

63
00:04:23,500 --> 00:04:28,500
第2の地殻変動は、教育、つまり研修手法そのものの変化ですね。

64
00:04:28,500 --> 00:04:35,000
「第3の研修手段としての、生成AIネイティブプラットフォームの登場」

65
00:04:35,000 --> 00:04:37,000
第3の手段？

66
00:04:37,000 --> 00:04:44,000
ええ、これまでの企業研修って、まあみんなで集まる集合研修か、ビデオを見るeラーニングのほぼ2択でした。

67
00:04:44,000 --> 00:04:46,000
はい、そうですね。

68
00:04:46,000 --> 00:04:53,000
そこに、AIと対話しながら実践練習ができるプラットフォームが、現実的な第3の選択肢として食い込んできたんです。

69
00:04:53,000 --> 00:05:01,000
資料には、AIが営業の顧客役になってくれるロールプレイングツールとか、

70
00:05:01,000 --> 00:05:06,000
教材を読み込ませるだけでテスト問題を自動生成するツール、なんて例が挙がってますね。

71
00:05:06,000 --> 00:05:13,000
この「顧客役AI」というのは、単に決まったセリフを返すだけのチャットボットとは違うんですか？

72
00:05:13,000 --> 00:05:15,000
全然違いますね。

73
00:05:15,000 --> 00:05:22,000
例えば、こっちが少し弱い態度を見せると急に強気な要求をしてきたりとか、

74
00:05:22,000 --> 00:05:26,000
そういう人間らしい駆け引きまでできるものなんですか？

75
00:05:26,000 --> 00:05:30,000
かなりそれに近いレベルになってきてます。単なる応答だけじゃなくて、

76
00:05:30,000 --> 00:05:37,000
「今のあなたの説明は専門用語が多すぎて顧客は理解できていませんよ、もっと平易な言葉で言い換えてください」みたいな、

77
00:05:37,000 --> 00:05:40,500
具体的なフィードバックまで返してくれます。

78
00:05:40,500 --> 00:05:42,000
へえー、すごい。

79
00:05:42,000 --> 00:05:46,000
まさに「練習装置」という言葉がぴったりで、

80
00:05:46,000 --> 00:05:51,000
これまでのeラーニングが知識をインプットする教科書だったのに対して、

81
00:05:51,000 --> 00:05:58,000
AIはアウトプットを徹底的に鍛える、部活の千本ノックとか、いつでも付き合ってくれる先輩みたいな存在に変わりつつあるんですよ。

82
00:05:58,000 --> 00:06:01,000
なるほど、部活の先輩、ですか。

83
00:06:01,000 --> 00:06:06,000
面白い表現ですね。気兼ねなく何度でも失敗できる相手がいるっていうのは、

84
00:06:06,000 --> 00:06:09,000
特に若手社員にとっては心強いでしょうね。

85
00:06:09,000 --> 00:06:11,500
ええ、本当にそう思います。

86
00:06:11,500 --> 00:06:17,000
でも、そういうAIコーチって、本当に人間の心の機微まで理解できるんでしょうか？

87
00:06:17,000 --> 00:06:23,000
例えば、本気で怒ってる顧客と、交渉のためにあえてプレッシャーをかけてきてる顧客の、

88
00:06:23,000 --> 00:06:27,000
その違いをAIは見抜いて、適切なフィードバックをくれるんですかね？

89
00:06:27,000 --> 00:06:33,000
そこがまさに現在の課題であり、同時に人間の講師の価値が高まるポイントでもあります。

90
00:06:33,000 --> 00:06:34,500
ああ、なるほど。

91
00:06:34,500 --> 00:06:39,000
先進的な企業では、基礎的なトーク練習とか知識の確認はAIに任せて、

92
00:06:39,000 --> 00:06:46,000
人間はより高度な、その場の空気を読んだり、相手の感情に寄り添ったりするような、

93
00:06:46,000 --> 00:06:51,000
そういう複雑な状況判断の指導に集中するっていう役割分担が見え始めています。

94
00:06:51,000 --> 00:06:52,000
はい。

95
00:06:52,000 --> 00:06:57,000
ただ、多くの企業ではまだ試験導入の段階でして、

96
00:06:57,000 --> 00:07:05,000
このAI練習で本当に社員の現場での行動は変わったのかっていう効果測定とか、

97
00:07:05,000 --> 00:07:11,000
それを人事評価とどう連携させるのかみたいな制度設計が追いついていないのが実情ですね。

98
00:07:11,000 --> 00:07:17,000
確かに、AIが個人の練習台になって、その出来不出来を評価し始めると、

99
00:07:17,000 --> 00:07:20,000
今度は別の問題が出てきますよね。

100
00:07:20,000 --> 00:07:21,000
ええ。

101
00:07:21,000 --> 00:07:27,000
その評価が、もし昇進とか異動に影響を与えるとしたら、それはかなりデリケートな話です。

102
00:07:27,000 --> 00:07:31,000
そうなると当然、厳格なルールが必要になってくる。

103
00:07:31,000 --> 00:07:34,000
これが3つ目のニュースですね。

104
00:07:34,000 --> 00:07:41,000
ええ、第3のプレートは「ルール」、すなわち法規制とガバナンスの話です。

105
00:07:41,000 --> 00:07:46,000
「AIガバナンス法制で変わる、AIリテラシー研修の中身」

106
00:07:46,000 --> 00:07:47,500
はい。

107
00:07:47,500 --> 00:07:54,000
特に影響が大きいのが、ヨーロッパで施行された「EU AI Act」です。

108
00:07:54,000 --> 00:08:01,000
この法律の興味深い点は、AI技術を「リスクのレベル」に応じて分類していることなんですが、

109
00:08:01,000 --> 00:08:08,500
その中で、教育、職業訓練や、採用、人事管理といった領域で使われるAIが

110
00:08:08,500 --> 00:08:12,000
「高リスクAI」に分類されたんです。

111
00:08:12,000 --> 00:08:14,000
高リスク、ですか？

112
00:08:14,000 --> 00:08:19,000
なぜ金融とか医療じゃなくて、人事が高リスクなんでしょう？

113
00:08:19,000 --> 00:08:29,000
それはですね、AIの判断が個人のキャリアとか採用機会に、直接的かつ重大な影響を与える可能性があるからなんです。

114
00:08:29,000 --> 00:08:30,000
ああ、なるほど。

115
00:08:30,000 --> 00:08:38,500
もしAIの学習データに過去の性別とか国籍による偏見が紛れ込んでいれば、

116
00:08:38,500 --> 00:08:43,000
それが無意識のうちに増幅されて、深刻な差別につながりかねない。

117
00:08:43,000 --> 00:08:48,000
だからこそ、EUは特に厳しい目を向けているわけです。

118
00:08:48,000 --> 00:08:49,000
はい。

119
00:08:49,000 --> 00:08:56,000
これにより、日本企業もEU圏内で事業を行う以上、このルールと無関係ではいられなくなりました。

120
00:08:56,000 --> 00:08:57,000
なるほど。

121
00:08:57,000 --> 00:09:05,000
つまり、人事部が研修や評価でAIを使うこと自体が、厳格なコンプライアンスの対象になったと。

122
00:09:05,000 --> 00:09:06,000
その通りです。

123
00:09:06,000 --> 00:09:11,500
そうなると、社員向けのAI研修の中身も当然変わってきますよね。

124
00:09:11,500 --> 00:09:19,000
はい。これまでは「会社のPCで機密情報を生成AIに入力しちゃいけません」といった、まあ禁止事項のリストを教えるのが中心でした。

125
00:09:19,000 --> 00:09:21,500
ええ、よくありますね。

126
00:09:21,500 --> 00:09:25,000
でも、これからはそれでは全く不十分です。

127
00:09:25,000 --> 00:09:30,500
もっと「実践的な判断力」を養うトレーニングが必須になります。

128
00:09:30,500 --> 00:09:33,000
実践的な、ですか？

129
00:09:33,000 --> 00:09:39,000
ええ、例えば「取引先からAIで生成した議事録案が送られてきた」

130
00:09:39,000 --> 00:09:44,500
「この内容をどこまで信用して社内報告に使っていいか」といった、

131
00:09:44,500 --> 00:09:50,000
白黒つけがたいグレーゾーンの事案に直面した時、どう考え、誰に相談すべきか。

132
00:09:50,000 --> 00:09:53,000
うーん、難しいですね、それは。

133
00:09:53,000 --> 00:09:59,000
そうなんです。そうしたケーススタディを通じて、社員一人一人の倫理観とかリスク感度を高めていく必要がある。

134
00:09:59,000 --> 00:10:07,000
研修担当者が、法務とか情報システム部門とこれまで以上に密に連携することが不可欠になったわけです。

135
00:10:07,000 --> 00:10:14,500
使う上でのルールを整備する一方で、そもそも「どのAIを使うか」という選択肢自体が爆発的に増えている、

136
00:10:14,500 --> 00:10:17,000
というのが4つ目の地殻変動ですね。

137
00:10:17,000 --> 00:10:18,500
はい。

138
00:10:18,500 --> 00:10:24,000
「第4のゲノーム」みたいな「トップモデル3強の拮抗」。

139
00:10:24,000 --> 00:10:28,500
少し前まで、生成AIといえばChatGPTという感じでしたが、

140
00:10:28,500 --> 00:10:35,500
資料によるとその一強時代は終わり、OpenAIのGPT系、GoogleのGemini系、

141
00:10:35,500 --> 00:10:41,000
AnthropicのClaude系が3強として競い合っていると。

142
00:10:41,000 --> 00:10:44,000
これはどういう状況なんでしょう？

143
00:10:44,000 --> 00:10:51,000
ここで重要なのは「どのモデルが一番か」っていう優劣の議論ではないんです。

144
00:10:51,000 --> 00:10:53,000
あ、そうなんですか。

145
00:10:53,000 --> 00:11:00,500
ええ。「得意なことが違う複数のモデルを、目的によって使い分けるのが当たり前になった」

146
00:11:00,500 --> 00:11:03,500
という、その事実そのものが重要なんです。

147
00:11:03,500 --> 00:11:06,500
これは大きな視点の転換ですね。

148
00:11:06,500 --> 00:11:08,000
なるほど。

149
00:11:08,000 --> 00:11:15,000
一つの万能ツールに頼るんじゃなくて、用途に合わせて最適な道具を選ぶ、という考え方です。

150
00:11:15,000 --> 00:11:16,500
なるほど。

151
00:11:16,500 --> 00:11:21,500
私のスマホでも、写真を整理するアプリと、文章を書くアプリと、

152
00:11:21,500 --> 00:11:26,500
地図を見るアプリが全部違うように、AIも万能選手一人に頼る時代じゃないと。

153
00:11:26,500 --> 00:11:28,000
まさに。

154
00:11:28,000 --> 00:11:32,000
資料には使い分けの例が載っていますが、例えば私がソフトウェア開発者なら、

155
00:11:32,000 --> 00:11:36,500
プログラムのコードレビューが得意なClaude系を使い、

156
00:11:36,500 --> 00:11:43,000
普段の業務がGoogleドライブとかGmail中心なら、それらとシームレスに連携するGeminiを使うのが自然。

157
00:11:43,000 --> 00:11:49,500
そして、何か特定の業務に特化したカスタムAIを作りたいならGPT系が強い、というようなイメージでしょうか。

158
00:11:49,500 --> 00:11:51,500
その理解で完璧です。

159
00:11:51,500 --> 00:11:56,000
そうなると、人材開発のテーマも変わってきますよね。

160
00:11:56,000 --> 00:11:57,500
確かに。

161
00:11:57,500 --> 00:12:02,000
以前は「ChatGPTの使い方講座」でよかった。

162
00:12:02,000 --> 00:12:07,500
でも今は、「我が社の業務プロセスとシステム環境を考えると、

163
00:12:07,500 --> 00:12:12,500
この業務ではGeminiを、こちらの業務ではClaudeを、

164
00:12:12,500 --> 00:12:17,500
このように連携させて使うのが最も生産性が上がります」という、

165
00:12:17,500 --> 00:12:22,000
より戦略的な視点で教える必要が出てきたんです。

166
00:12:22,000 --> 00:12:24,500
うわあ、それは大変ですね。

167
00:12:24,500 --> 00:12:30,000
教える側である人事とか研修の担当者が、まずこの目まぐるしいマルチモデル環境の進化を

168
00:12:30,000 --> 00:12:33,500
常にキャッチアップし続けなきゃいけない。

169
00:12:33,500 --> 00:12:37,000
これは新しいプレッシャーじゃないですか？

170
00:12:37,000 --> 00:12:38,500
おっしゃる通りです。

171
00:12:38,500 --> 00:12:43,500
もはや研修担当者は、外部の講師を呼んでくるだけでは務まりません。

172
00:12:43,500 --> 00:12:49,000
自らが各AIの特性を試して理解して、自社の業務に合わせた最適な活用法を設計する

173
00:12:49,000 --> 00:12:56,500
「AI活用ストラテジスト」のような役割を担うことが求められ始めている。

174
00:12:56,500 --> 00:13:00,000
これもまた大きな変化の一つです。

175
00:13:00,000 --> 00:13:07,000
さて、ここまで「効率化」「教育」「ルール」「選択肢」と、4つのプレートの動きを見てきました。

176
00:13:07,000 --> 00:13:14,000
最後の5つ目は「表現方法」、つまりナレッジの共有の仕方の変化ですね。

177
00:13:14,000 --> 00:13:15,500
ええ。

178
00:13:15,500 --> 00:13:21,500
「マルチモーダル技術の進化で、社内教材の内製化が現実に」というニュース。

179
00:13:21,500 --> 00:13:24,500
特に動画の話ですね。

180
00:13:24,500 --> 00:13:30,500
Google Vidsというツールは、資料と簡単な指示からトレーニング動画の初稿を自動で生成してくれるとか。

181
00:13:30,500 --> 00:13:32,000
はい。

182
00:13:32,000 --> 00:13:40,000
スイスの銀行UBSが、アナリストのAIアバターを使ってリサーチ動画の配信本数を5倍にしようとしているとか。

183
00:13:40,000 --> 00:13:48,000
え、これって私が昨日作ったプレゼン資料を放り込んだら、今晩には私そっくりのアバターが喋る解説動画が出来上がってる、

184
00:13:48,000 --> 00:13:51,500
みたいな世界がもう来ているということですか？

185
00:13:51,500 --> 00:13:55,000
はい、まさにそういう世界です。

186
00:13:55,000 --> 00:14:02,000
これにより、これまで動画制作の最大のハードルだった撮影や編集の手間が劇的に下がりました。

187
00:14:02,000 --> 00:14:03,500
うーん。

188
00:14:03,500 --> 00:14:07,500
その結果、課題が180度変わったんです。

189
00:14:07,500 --> 00:14:14,000
これまでは「どうやって作るか」が問題でしたが、今は「大量に生まれる動画をどう管理し、どう活用するか」が

190
00:14:14,000 --> 00:14:19,000
新たな、そしてより複雑な課題になっています。

191
00:14:19,000 --> 00:14:20,500
ああ、なるほど。

192
00:14:20,500 --> 00:14:28,000
誰でも簡単に作れるようになったからこそ、質の低い動画とか、内容が重複した動画が社内に溢れかえってしまう危険があるわけですね。

193
00:14:28,000 --> 00:14:30,000
その通りです。

194
00:14:30,000 --> 00:14:34,000
そして、もっと深刻なリスクもあります。

195
00:14:34,000 --> 00:14:36,000
と言いますと？

196
00:14:36,000 --> 00:14:44,000
例えば、ある社員が現場のノウハウを共有しようと、親切心でスマホで撮影した動画に、

197
00:14:44,000 --> 00:14:49,000
たまたま背景のホワイトボードに書かれた顧客の機密情報が写り込んでいたとしたら。

198
00:14:49,000 --> 00:14:52,000
うわあ…。

199
00:14:52,000 --> 00:14:58,000
その動画が承認プロセスを経ずに全社に共有されてしまったら、誰が責任を取るのか。

200
00:14:58,000 --> 00:15:06,000
こうした事態を防ぐための「動画ガバナンス」の設計が、人事や研修担当の全く新しい仕事として生まれているんです。

201
00:15:06,000 --> 00:15:08,000
動画ガバナンスですか。

202
00:15:08,000 --> 00:15:15,500
具体的には、動画に登場する人物の肖像権の扱とか、承認フローの整備、古い情報の削除ルールといったことでしょうか。

203
00:15:15,500 --> 00:15:17,000
ええ。

204
00:15:17,000 --> 00:15:21,500
そうしたルール作りと同時に、意識改革も起きています。

205
00:15:21,500 --> 00:15:27,000
先進的な企業では、もはや研修動画は専門部署が外注して作るものではありません。

206
00:15:27,000 --> 00:15:28,000
はい。

207
00:15:28,000 --> 00:15:35,500
現場の担当者が、日々の気づきをAIの支援を受けてさっと動画にし、組織全体に展開するものへと変わりつつある。

208
00:15:35,500 --> 00:15:43,000
現場の「暗黙知」を、組織の「形式知」に変えるための最も強力なツールとして、動画が再定義されているんです。

209
00:15:43,000 --> 00:15:44,500
なるほど。

210
00:15:44,500 --> 00:15:49,000
さて、5つの大きな地殻変動を見てきました。

211
00:15:49,000 --> 00:15:58,000
AIが全社員の教養になり、AIとの対話が第3の学び方になり、その活用には厳格な法規制が求められ、

212
00:15:58,000 --> 00:16:05,000
使うべきAIの選択肢は多様化、そして知識を共有する表現方法として動画が民主化された。

213
00:16:05,000 --> 00:16:06,000
ええ。

214
00:16:06,000 --> 00:16:11,500
まさに、働き方の前提が根こそぎ変わってしまった感じがしますね。

215
00:16:11,500 --> 00:16:17,500
資料の最後には、企業の人材開発担当者向けのチェックリストが載っています。

216
00:16:17,500 --> 00:16:24,500
自社のAIスキルレベルは定義されているか、とか、AIロールプレイを試したことがあるか、といった項目です。

217
00:16:24,500 --> 00:16:29,000
ええ、そのチェックリストはあくまで企業からの視点ですよね。

218
00:16:29,000 --> 00:16:30,000
はい。

219
00:16:30,000 --> 00:16:35,000
最後に、これをぜひご自身の視点に置き換えて考えてみてほしいんです。

220
00:16:35,000 --> 00:16:36,000
ほう。

221
00:16:36,000 --> 00:16:42,000
今日お話しした5つの変化の中で、来年、ご自身の仕事やキャリアに

222
00:16:42,000 --> 00:16:46,000
最も大きな影響を与えそうなのはどれだと思いますか？

223
00:16:46,000 --> 00:16:47,500
うーん。

224
00:16:47,500 --> 00:16:53,000
全社員がAIを学ぶというリスキリングの圧力でしょうか。

225
00:16:53,000 --> 00:16:57,500
あるいは、AIコーチと練習する新しい学び方の習慣でしょうか。

226
00:16:57,500 --> 00:17:05,000
はたまた、複数のAIを巧みに使い分けるマルチモデルのスキルや、自分の知識や経験を動画で発信する能力が、

227
00:17:05,000 --> 00:17:09,500
ご自身の市場価値を左右することになるのでしょうか。

228
00:17:09,500 --> 00:17:11,000
なるほど。

229
00:17:11,000 --> 00:17:15,500
この問いを、ぜひご自身の未来を考えるきっかけにしてください。
